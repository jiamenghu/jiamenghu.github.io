<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-pytorch学习笔记" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/10/07/pytorch学习笔记/" class="article-date">
  <time datetime="2019-10-07T12:23:58.233Z" itemprop="datePublished">2019-10-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>title: pytorch学习笔记（一）</p>
<h1 id="pytorch笔记"><a href="#pytorch笔记" class="headerlink" title="pytorch笔记"></a>pytorch笔记</h1><hr>
<p>基于python的科学计算软件包<br>Pytorch和TensorFlow的区别：  </p>
<ul>
<li>TensorFlow是基于静态计算图的，静态计算图是先定义后运行，一次定义多次运行（Tensorflow 2.0也开始使用动态计算图）  </li>
<li>PyTorch是基于动态图的，是在运行的过程中被定义的，在运行的时候构建，可以多次构建多次运行  </li>
</ul>
<h2 id="基础入门"><a href="#基础入门" class="headerlink" title="基础入门"></a>基础入门</h2><h3 id="张量-Tensor"><a href="#张量-Tensor" class="headerlink" title="张量 Tensor"></a>张量 Tensor</h3><p>tensor的数据类型：<br>32 位浮点型 torch.Float Tensor、<br>64 位浮点型 torch.DoubleTensor、<br>16 位整型 torch.Shor tTensor、<br>32 位 整型 torch.lntTensor、<br>64 位整型 torch.LongTensor、<br>torch.Tensor 默认的是 torch.FloatTensor 数据类型。  </p>
<p>ytorch中的Tensor和ndarray类似，区别在于ndarray不能再GPU上加速，而Tensor可以使用GPU加速</p>
<p>使用张量构造一个未初始化的5*3的矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.empty(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[9.0919e-39, 1.0102e-38, 8.9082e-39],
        [9.9184e-39, 9.0000e-39, 9.2755e-39],
        [8.9082e-39, 9.9184e-39, 9.0000e-39],
        [1.0561e-38, 4.9593e-39, 1.0010e-38],
        [1.0010e-38, 4.2246e-39, 1.1112e-38]])</code></pre><p>构造一个随机数据的矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = torch.rand(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0.5333, 0.0055, 0.2235],
        [0.8860, 0.9165, 0.7020],
        [0.7442, 0.3467, 0.2753],
        [0.3033, 0.7016, 0.6224],
        [0.2453, 0.7622, 0.5449]])</code></pre><p>构造一个dtype为long的全0矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(<span class="number">5</span>,<span class="number">3</span>,dtype=torch.long)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])</code></pre><p>直接通过数据来构造张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.5</span>,<span class="number">3</span>])</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([5.5000, 3.0000])</code></pre><p>或基于已有张量来创建张量。除非用户提供新的值，否则这些方法将重用输入张量的属性，例如dtype</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = x.new_ones(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">x = torch.randn_like(x,dtype=torch.float)   <span class="comment">#重写数据类型</span></span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
tensor([[-1.2661,  0.2785,  0.5638],
        [ 1.5987,  0.2342,  1.5551],
        [ 0.8861,  1.1387, -2.3570],
        [-0.4471, -0.4448,  1.5366],
        [-0.4152,  0.8103, -1.8847]])</code></pre><p>获取张量的形状<code>shape</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.size()) <span class="comment">#输出torch.size是一个tuple元组</span></span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([5, 3])</code></pre><p>调整数据shape：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.view(<span class="number">-1</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-1.2661,  0.2785,  0.5638,  1.5987,  0.2342],
        [ 1.5551,  0.8861,  1.1387, -2.3570, -0.4471],
        [-0.4448,  1.5366, -0.4152,  0.8103, -1.8847]])</code></pre><h3 id="运算-Operations"><a href="#运算-Operations" class="headerlink" title="运算 Operations"></a>运算 Operations</h3><p>加法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line">y = torch.ones(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">print(y)    </span><br><span class="line"><span class="comment">#加法第一种操作:</span></span><br><span class="line">print(x+y)</span><br><span class="line"><span class="comment">#加法第二种操作:</span></span><br><span class="line">torch.add(x,y)</span><br><span class="line"><span class="comment">#加法第三种操作: 提供一个输出张量作为参数</span></span><br><span class="line">result=torch.empty(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">torch.add(x,y,out=result)</span><br><span class="line">print(result)</span><br><span class="line"><span class="comment">#加法第四种操作:</span></span><br><span class="line">y.add_(x) <span class="comment">#任何运算方法名后面加“_”都代表in-place。例如：x.copy_(y),x.t_(),都会改变x。</span></span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]])
tensor([[2., 2., 2.],
        [2., 2., 2.],
        [2., 2., 2.],
        [2., 2., 2.],
        [2., 2., 2.]])





tensor([[2., 2., 2.],
        [2., 2., 2.],
        [2., 2., 2.],
        [2., 2., 2.],
        [2., 2., 2.]])</code></pre><p>调整大小：如果你想要改变张量的形状，可以使用torch.view:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x=torch.randn(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">y=x.view(<span class="number">16</span>)</span><br><span class="line">z=x.view(<span class="number">-1</span>,<span class="number">8</span>)<span class="comment">#-1代表通过其它维度推断获取</span></span><br><span class="line">print(x.size(),y.size(),z.size())</span><br><span class="line">print(x)</span><br><span class="line">print(y)</span><br><span class="line">print(z)</span><br></pre></td></tr></table></figure>

<pre><code>torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
tensor([[-0.3895, -0.2331,  0.0171, -0.0803],
        [ 0.8870,  0.5028, -0.0394,  0.1140],
        [ 0.2444, -0.0775,  1.0308, -0.4988],
        [-1.8849, -1.1274, -0.1732, -0.3120]])
tensor([-0.3895, -0.2331,  0.0171, -0.0803,  0.8870,  0.5028, -0.0394,  0.1140,
         0.2444, -0.0775,  1.0308, -0.4988, -1.8849, -1.1274, -0.1732, -0.3120])
tensor([[-0.3895, -0.2331,  0.0171, -0.0803,  0.8870,  0.5028, -0.0394,  0.1140],
        [ 0.2444, -0.0775,  1.0308, -0.4988, -1.8849, -1.1274, -0.1732, -0.3120]])</code></pre><p>如果张量只有一个元素，使用.item()可以获取它的Python数字的值:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x=torch.randn(<span class="number">1</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x.item())</span><br></pre></td></tr></table></figure>

<pre><code>tensor([-0.5732])
-0.5731935501098633</code></pre><h4 id="Torch的张量和Numpy的数组将共享底层的内存位置，更改一个另一个也会改变。"><a href="#Torch的张量和Numpy的数组将共享底层的内存位置，更改一个另一个也会改变。" class="headerlink" title="Torch的张量和Numpy的数组将共享底层的内存位置，更改一个另一个也会改变。"></a>Torch的张量和Numpy的数组将共享底层的内存位置，更改一个另一个也会改变。</h4><p>Torch的张量到Numpy数组的转换:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a=torch.ones(<span class="number">5</span>)</span><br><span class="line">print(a)</span><br><span class="line"> </span><br><span class="line">b=a.numpy()</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([1., 1., 1., 1., 1.])
[ 1.  1.  1.  1.  1.]</code></pre><p>torch的张量改变时numpy数组也随着改变:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a.add_(<span class="number">1</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([2., 2., 2., 2., 2.])
[ 2.  2.  2.  2.  2.]</code></pre><p>Numpy数组转换为Torch张量:(改变Numpy数组时Torch张量也跟着改变)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a=np.ones(<span class="number">5</span>)</span><br><span class="line">b=torch.from_numpy(a)</span><br><span class="line">np.add(a,<span class="number">1</span>,out =a)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>

<pre><code>[ 2.  2.  2.  2.  2.]
tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</code></pre><p>在CPU上所有的张量（CharTensor除外）均支持与Numpy的转换。</p>
<h2 id="自动微分-autograd"><a href="#自动微分-autograd" class="headerlink" title="自动微分 autograd"></a>自动微分 autograd</h2><p>autograd package是PyTorch神经网络的核心<br>autograd package为张量的所有operations（操作或运算）提供了自动微分。它是一个 define-by-run框架，意思是说你的反向传播（backpropagation）是由 如何运行代码 定义的，并且每一个迭代可以是不同的。</p>
<p>torch.Tensor是包的核心类。如果你设置它的<code>.requires_grad</code>属性 为 True，它的所有操作都会被跟踪记录。完成计算后可以调用<code>.backward()</code>方法，并自动计算所有的梯度。这个张量的梯度将会被累积到.grad属性。</p>
<p>可以调用<code>.detach()</code>方法来阻止张量对所有操作的跟踪记录</p>
<p>阻止跟踪历史的另一种方法是你在代码块外包裹语句 <code>with torch.no_grad():</code> ，这种方法在模型中训练参数设置了<code>requires_grad=True</code>时依然有效。</p>
<p>实现自动微分的另一个重要的类是<code>Function</code>.(这里function可理解为一个运算。比如加法运算)</p>
<p>Tensor 和 Function相互关联建立了一个非循环图，它记录了完整的计算历史。每一个张量有一个<code>.grad_fn</code>属性，这个属性与创建张量（除了用户自己创建的张量，它们的<code>.grad_fn</code>是<code>None</code>）的<code>Function</code>关联。</p>
<p>如果你想要计算导数，你可以调用张量的<code>.backward()</code>方法。如果张量是一个标量（例如，它只有一个元素），你不需要指定<code>backward()</code>的参数，然而，如果张量有多个元素，你需要指定一个匹配张量大小的<code>gradient</code>参数。</p>
<p>怎么用</p>
<ul>
<li><p>跟踪Tensor上的所有操作 : 设置属性<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">* 自动计算所有梯度 ： 调用```.backward()</span><br></pre></td></tr></table></figure></p>
</li>
<li><p>停止跟踪Tensor： 调用<figure class="highlight plain"><figcaption><span>或者代码块 `with torch.no grad`</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">* 若Tensor不仅仅是标量，则需要指定`gradient`参数来指定张量的形式</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">import torch</span><br></pre></td></tr></table></figure></p>
</li>
</ul>
<p>创建一个张量，并设置<code>requires_grad=True</code>来跟踪计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x =torch.ones(<span class="number">2</span>,<span class="number">2</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x.requires_grad)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
True</code></pre><p>对张量做一个加法运算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y = x+<span class="number">2</span></span><br><span class="line">print(y)</span><br><span class="line">print(y.grad_fn)  <span class="comment">#y是被刚刚的运算（+）创建的，所以它有.grad_fn属性</span></span><br></pre></td></tr></table></figure>

<pre><code>tensor([[3., 3.],
        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)
&lt;AddBackward0 object at 0x00000208C0C1F8D0&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">z=y*y*<span class="number">3</span></span><br><span class="line">out=z.mean()</span><br><span class="line">print(z,z.grad_fn)</span><br><span class="line">print(out,out.grad_fn)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[27., 27.],
        [27., 27.]], grad_fn=&lt;MulBackward0&gt;) &lt;MulBackward0 object at 0x00000208C0DE5470&gt;
tensor(27., grad_fn=&lt;MeanBackward0&gt;) &lt;MeanBackward0 object at 0x00000208C0DE2860&gt;</code></pre><p><code>.requires_grad_(...)</code>函数可以改变张量的<code>requires_grad</code>属性。如果该属性未给出，则默认为<code>False</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a=torch.randn(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">a=((a*<span class="number">3</span>)/(a<span class="number">-1</span>))</span><br><span class="line">print(a.requires_grad)  <span class="comment"># 默认False</span></span><br><span class="line">a.requires_grad_(<span class="literal">True</span>)  <span class="comment"># 打开跟踪</span></span><br><span class="line">print(a.requires_grad)</span><br><span class="line">b=(a*a).sum()</span><br><span class="line">print(b.grad_fn)</span><br></pre></td></tr></table></figure>

<pre><code>False
True
&lt;SumBackward0 object at 0x00000208C0C1FCF8&gt;</code></pre><h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>因为out只是一个简单的标量，out.backward()等价于out.backward(torch.tensor(1))</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out.backward()</span><br></pre></td></tr></table></figure>

<p>打印出导数 d(out)/dx</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])</code></pre><p>$$ out = \frac{1}{4}\sum_i z_i$$，其中$z_i = 3(x_i+2)^2$，因为方向传播中torch.tensor=1（out.backward中的参数）因此$z_i\bigr\rvert_{x_i=1} = 27$</p>
<p>对于梯度$\frac{\partial out}{\partial x_i} = \frac{3}{2}(x_i+2)$，把$x_i=1$代入$\frac{\partial out}{\partial x_i}\bigr\rvert_{x_i=1} = \frac{9}{2} = 4.5$</p>
<p>对标量进行backward(),不需要参数。下面看下是向量或矩阵时，gradient参数的值对梯度的影响：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x=torch.randn(<span class="number">3</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(x)</span><br><span class="line">y=x*<span class="number">2</span></span><br><span class="line"><span class="keyword">while</span> y.data.norm()&lt;<span class="number">1000</span>:</span><br><span class="line">    y=y*<span class="number">2</span></span><br><span class="line"><span class="comment">#print(y)</span></span><br><span class="line">gradients = torch.tensor([<span class="number">0.1</span>,<span class="number">1.0</span>,<span class="number">0.0001</span>],dtype=torch.float)</span><br><span class="line">y.backward(gradient=gradients)</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([ 1.5415, -0.6833,  0.2114], requires_grad=True)
tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])</code></pre><p>如果你不想要跟踪历史并自动微分的时候，你可以在代码块外面包一层：with torch.no_grad():</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(x.requires_grad)</span><br><span class="line">print((x**<span class="number">2</span>).requires_grad)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    print((x**<span class="number">2</span>).requires_grad)</span><br></pre></td></tr></table></figure>

<pre><code>True
True
False</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/10/07/pytorch学习笔记/" data-id="ck1gffqze0002kwvbgaqyx00z" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-firstblog" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/10/06/firstblog/" class="article-date">
  <time datetime="2019-10-06T13:58:39.000Z" itemprop="datePublished">2019-10-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/10/06/firstblog/">firstblog</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="看啥呀，这什么也没有"><a href="#看啥呀，这什么也没有" class="headerlink" title="看啥呀，这什么也没有"></a>看啥呀，这什么也没有</h1><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cout</span>&lt;&lt;<span class="string">"fuck you"</span></span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/10/06/firstblog/" data-id="ck1gffqz20001kwvb9jns8z5c" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/10/07/pytorch学习笔记/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/10/06/firstblog/">firstblog</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>